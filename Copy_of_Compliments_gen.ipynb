{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Compliments_gen",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ltdiadams/Compligenix/blob/master/Copy_of_Compliments_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Compliments Generation\n",
        "\n",
        "\n",
        "This file will help you train on data to generate similar data for you. In our case we have given it compliments data.\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes.\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Upload the training data file in the files section of coolab.\n",
        "4. Run the cells below to set up the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "2986d990-5710-4fcd-f381-02ad17b3be89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "!pip install -q textgenrnn\n",
        "from google.colab import files\n",
        "from textgenrnn import textgenrnn\n",
        "from datetime import datetime\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "If you are using an input file where documents are line-delimited, make sure to set `line_delimited` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cfg = {\n",
        "    'word_level': False,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
        "    'rnn_size': 128,   # number of LSTM cells of each layer (128/256 recommended)\n",
        "    'rnn_layers': 3,   # number of LSTM layers (>=2 recommended)\n",
        "    'rnn_bidirectional': False,   # consider text both forwards and backward, can give a training boost\n",
        "    'max_length': 30,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
        "    'max_words': 10000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': True,   # set to True if each text has its own line in the source file\n",
        "    'num_epochs': 100,   # set higher to train the model for longer\n",
        "    'gen_epochs': 5,   # generates sample text from model after given number of epochs\n",
        "    'train_size': 0.8,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
        "    'dropout': 0.0,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
        "    'validation': False,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
        "    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"compliments.txt\"\n",
        "model_name = 'compliments'   # change to set file name of resulting trained models/texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "The next cell will start the actual training.\n",
        "Ideally, you want a training loss less than `1.0` in order for the model to create sensible text consistently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "d1456b58-4bda-48db-d2ee-154ef22e14df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file_name,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=1024,\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=100,\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "1,097 texts collected.\n",
            "Training new model w/ 3-layer, 128-cell LSTMs\n",
            "Training on 37,549 character sequences.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "36/36 [==============================] - 8s 228ms/step - loss: 3.3731\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 3.0716\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 3.0669\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.0649\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.0624\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "                        o               o       e                 o              o               o              e     o                  e                                                                    n                 e  o              o     e            t                      eo         e  \n",
            "\n",
            "                                         o        e    ee                         a  e                               u          .                i          a                          i       e    o  t                                e                         a                                  .o   \n",
            "\n",
            "          o   o o                                                e              e                                       o o                                                                              e                      n                                                                         \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "oeu ie    e    ea  oyaooi oa gh r  n ee  n   o   s n t oem    a oa  aiieolf tt eneoe  .rit i i \n",
            "\n",
            "ahuaaoor eIle o   r e o ae   eo   uo  hehya iwrtlhioetnteote  u   \n",
            "\n",
            "  o  eroo  o  o elr e  onor yl r pte\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "\n",
            "\n",
            "oiyaakl ix tyyaleuisCe uoY \n",
            "\n",
            " aaio o e s mrOrmtar ea.assritem yfoteitthonrt \n",
            "\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 3.0612\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 3.0568\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.0548\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.0351\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.9637\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Yo  o   o o o   eoe  t        o    i\n",
            "\n",
            "Yu        e     e n      t      a  e ott ete   t  e  te\n",
            "\n",
            "Y ou    n            a    t  oo     t       te neo    tt tt  ittt t  s e ee        tte   ntt    . t  ot   ti tteee  t teo i .  aeteo \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Yaooooonieloitane yisa n ealtctsf et rse etemre es  eei p t.i n  e. ln   easaytur tt ieae \n",
            "\n",
            "Y rt e a na ttg u on   gey eturl o   te.e n n rnbnotiroiet  . f iree  etrv o.etiptaaite .i nt tidit.. sa\n",
            "\n",
            "I   l  o   it  t o    t   teei o.strnc fn dtilmotna uibtst t e die aefte etaeioehrhnoof. ea ine d.lt l ada ta ee o kb  iteee.eat..rht \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "W  ogosynh t mfi\n",
            "\n",
            "Y a it tmd ti ua\n",
            "\n",
            "Ioeohcn et ly yo  knuoleuru.ew.\n",
            "\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 2.8987\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.8983\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.6388\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.2894\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.1391\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are an an an and tou and an and you are an at an and at are and and at ave you at ave at and an an you are ware and and at you and and are so an an at ang to are ave to and and the ave and at an at are an at and ave an ave and an at are and are you and ave ave at you at and an an and and and a\n",
            "\n",
            "You are ang at an ave are at at ave at an you are and and at and you an at you ave ave at an an tho are and and and at at you at are the and and at at and you are and an at an and at at are to and an and an an ing and and and and ing and at are the and you the and at at you the at hat and and at o\n",
            "\n",
            "You are are sot and and our at you are at at are an aly an at an at the ang and an are at and alis an the and and the and ang at at at at at an at are at at an and ave ave and at aly and and and and at ave and ave at and an ave ave are ang are ang our and ave to are the ale an an at are and are an\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You ave te at at alw I you pralle you beis soelin ave the ave and at ond to ate you loind as calw you.\n",
            "\n",
            "You are sea seos so poou to ad the at oud aberling at you af ouf ave the at are soco maly soning ave t are you ing aly wala ing owat o that toun meang you at to fo in ane and al at ang tho ing on aver you cou alwing may ve wat me bele the b al ang are you to the oun and an ato ay at you aly meat a\n",
            "\n",
            "I ve ss ou be an ouve ave at at you mpes ing you an at to your on meralds ing you.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You whises any gibut, corme ak sven te peingeten.\n",
            "\n",
            "You ase me sst os athang too be ave aninelise onow gomey fortend'xatezik.\n",
            "\n",
            "You make a som you ond.\n",
            "\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 2.0346\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 1.9626\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 1.8983\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 1.8159\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 1.7669\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so that an and you are and you are and you are and and and you are alway the to be and you are are and you have are the you are and and you are are are you are alway and you are alive rever you are are alway you are alway you are and you are are and you so walk you are are an you are and t\n",
            "\n",
            "You are so me so me and and to me and you are are an your alway to have you.\n",
            "\n",
            "You are so mally do be alway to an you are are and you are are and you are an you.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are so great a do and do do pretive an you ave you.\n",
            "\n",
            "You are a wand you rake fare be your abes are like you.\n",
            "\n",
            "You have so be exples ento to work at me mel you the wath you are deam you.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "It aner be met your to dieve th tom the dard vemer. You \n",
            "\n",
            "I lolve exprele quur you.\n",
            "\n",
            "The’dine to knocakis?\n",
            "\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.6925\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 2s 55ms/step - loss: 1.6484\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.5848\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 1.5301\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.4733\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so cound and who be could the best of the best to be a to be couth and the best a treat to me the best in the mors and and a best to be a be so me.\n",
            "\n",
            "You are so cand a best to have the best and the worl the best a the best to be and to make comm that the best to leary to have a to me a comporn and the best to have you are an the best and the best to have you are an the best to be a compormable and the best and a to me sell the best and the best\n",
            "\n",
            "You are so best a betting the best and the best and the best to be a be comple for the best and the best to be a comple and a to be the bost and the best and the best to be cormess and a dome the best and the best to me the best to have you are are in the best and the best and the worl the best an\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You’re a great how so man do somest and great to feally in mile beativar to the be somest and that you really goud the bove and you are lough.\n",
            "\n",
            "I love the comple so me somel.\n",
            "\n",
            "I love your look insmithous that to cand as your best word and compect and when you are in is tille not gore.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Me luck I com ary hadry to deall thable the somhivates.\n",
            "\n",
            "Your's so conkiting at wen you al, to babe cove. \n",
            "\n",
            "You're foll if carnice mefly.\n",
            "\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.4257\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 1.3790\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 1.3322\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 1.2741\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 1.2343\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are such a gor a better person to a that you are are always and the worlly the worlly day.\n",
            "\n",
            "You are such a good person with you.\n",
            "\n",
            "You are such a gor a bester person.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You’re a great and prond when you are.\n",
            "\n",
            "You are so cuch a bable perfon and is with shands.\n",
            "\n",
            "You look a gothing show the creating worm in the best and all and and life.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You ise a idener when your anoth thowaken you.\n",
            "\n",
            "You rees a besange up perfect in loth I have jot your crive.\n",
            "\n",
            "I love you make be a zest in awile the life the preasking and wind in thing your wokntionqe ard.\n",
            "\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 1.1883\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.1401\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.0994\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.0500\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 1.0042\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I love your person.\n",
            "\n",
            "I love your senfert.\n",
            "\n",
            "You are so brave and person.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "I love your selflesss.\n",
            "\n",
            "You are so breat.\n",
            "\n",
            "You are so lust to dandersting things and as your socets intidenes the world the best and wor.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Our breatiess persone of mime rowfest.\n",
            "\n",
            "I apspait love in ther you’ve.\n",
            "\n",
            "You always how that you arouadly eacor.\n",
            "\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.9600\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.9181\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.8664\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.8257\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.7757\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so handy.\n",
            "\n",
            "You are so handy.\n",
            "\n",
            "You are so handy.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You make me a way to be a your hire.\n",
            "\n",
            "You are so cool.\n",
            "\n",
            "You are so cool.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Whes an the rest coring I’m life. You look be is incredible.\n",
            "\n",
            "You are so handy.\n",
            "\n",
            "You’re trean.\n",
            "\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.7980\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.7270\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.6859\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.6392\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.6178\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are a really good prompoons in the mort inteligent poods of your way.\n",
            "\n",
            "You are a really good prompoons in the mort inteligent poods out of your work is.\n",
            "\n",
            "You are so hardworking and comition. You have it emproon.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are a real in proop to me take the beet some possile and completion.\n",
            "\n",
            "You are a gordinable guy.\n",
            "\n",
            "You are wise beyond your your tients.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "There's outtinging gy guillspences.\n",
            "\n",
            "Smome you outiling always without setting the worlls with to you.\n",
            "\n",
            "You are are such reason speininy lifeline.\n",
            "\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.5578\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5420\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5115\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4812\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.5159\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so something style. I approove you.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "You are so strong.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You're a great example for you.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "You are so sidester at excellent from you.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You are my fulp than a bites mesk me.\n",
            "\n",
            "This qual, you do be loke when you feel lest codvicalf.!\n",
            "\n",
            "You are the funnigent veruct you because you deeleviothice.\n",
            "\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.4524\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4074\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3892\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3629\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3518\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so doniong.\n",
            "\n",
            "You are so reliable.\n",
            "\n",
            "You are so cool.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are so suave.\n",
            "\n",
            "You are so droworsty.\n",
            "\n",
            "You are my cool.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "I appreciate th what you know how to car. \n",
            "\n",
            "You could is impecceccable.\n",
            "\n",
            "You feel were better about in the ligher gillenned.\n",
            "\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.3625\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.3159\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.2972\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3361\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.2775\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You have such a kind suce perspectable.\n",
            "\n",
            "You are so donionary, and I kends as a parat learry frifents and I see fel day.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are so brave.\n",
            "\n",
            "You have such a great smile.\n",
            "\n",
            "You are a really good problem-somininy to time with you.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You make me laugh.\n",
            "\n",
            "You mast abeveaule you are.\n",
            "\n",
            "Your exy is hard work.\n",
            "\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.2668\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.2647\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.2980\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.2466\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.2423\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so reliable.\n",
            "\n",
            "You are such a gentle soul.\n",
            "\n",
            "You are such a gentle soul.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are really handsome.\n",
            "\n",
            "You are such a hunk.\n",
            "\n",
            "You are such a hunk.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Conowing you, I’m and is abrinic.\n",
            "\n",
            "You're so atlenting person.\n",
            "\n",
            "You have a boted to herpest person other plows.\n",
            "\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.2360\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.2433\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.2659\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.2255\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.2206\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are such a hunk.\n",
            "\n",
            "You are so handsome.\n",
            "\n",
            "You are so handsome.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You have such a kind soursh.\n",
            "\n",
            "You are such an easyging with our a daffers.\n",
            "\n",
            "You are so handsome.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Your work ethic speaks for itself.\n",
            "\n",
            "I miss you even wher you do something elese a hap you have come me.\n",
            "\n",
            "You carn amle smart in that interys tame that meses my friend.\n",
            "\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.2186\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.2171\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.2137\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.2137\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.2610\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are such a helpful to tho rearding partture Ind than you want indieve not many to learr so without you.\n",
            "\n",
            "You are a real go-dentual.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are so athletic.\n",
            "\n",
            "You are always so energett.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Your habless me every single advor you are.\n",
            "\n",
            "That coronideding wel lourth you look qeuth realcesh, I know you.\n",
            "\n",
            "You are unote is life.\n",
            "\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.2077\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.2026\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 2s 55ms/step - loss: 0.2017\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1997\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1984\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so suave.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "I love how you patiep to feer and hour to you.\n",
            "\n",
            "You are a dedicated employee.\n",
            "\n",
            "You are so irresistibly charming.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You are, when you’re were hard all my lesfly anyter genteng on our kind(s). That you hase babe maker I meeed for a keil.\n",
            "\n",
            "You have such an energetic personal.\n",
            "\n",
            "You have exher person of kipssile.\n",
            "\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.1968\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1961\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1944\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1936\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.1915\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so handy!\n",
            "\n",
            "You are a real go-dentual.\n",
            "\n",
            "You are so smart.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are the most responsible guy is rome me.\n",
            "\n",
            "You are a real go-dentual.\n",
            "\n",
            "I am so lucky to have you around.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You're all that and a super-size bag of chips.\n",
            "\n",
            "Your hame sthing thap is breand for me ore with whotoubles you con’t macing  be a strong a motle met you are — your moind.\n",
            "\n",
            "You’re a great smire.\n",
            "\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.1905\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1893\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.1878\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.1862\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.1845\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so suave.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are such an inspiration to your tal.\n",
            "\n",
            "You're more helpful than a unicautsful.\n",
            "\n",
            "You are such an attentime great.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You are such an attentime graunderful.\n",
            "\n",
            "You are such an easygoing guy.\n",
            "\n",
            "You have great hair that in shole.\n",
            "\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.1841\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 2s 55ms/step - loss: 0.1819\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.1815\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.1794\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.1786\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so suave.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "You are so smart.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are so resole.\n",
            "\n",
            "You are such a strong paring paycect.\n",
            "\n",
            "I can't thank you takes at the freast on it.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Your capable commaries.\n",
            "\n",
            "I can’t gets hore outlally and poboraly.\n",
            "\n",
            "You are so und a hard.\n",
            "\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.1776\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.1770\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.1755\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.1735\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.1723\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "You are so smart.\n",
            "\n",
            "You are so handy.\n",
            "\n",
            "You are so suave.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You are so easy to have a converfethouf.\n",
            "\n",
            "You are such a hard worker.\n",
            "\n",
            "You have good rtast seees on I would facliman for style.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "You’re my worr.\n",
            "\n",
            "I rey a ster seen timing with you as emplesssious.\n",
            "\n",
            "You’re outgigeny and a duursialal when you laave achefuring the good you.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "You can download a large amount of generated text from your model with the cell below! Rerun the cell as many times as you want for even more text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this temperature schedule cycles between 1 very unexpected token, 1 unexpected token, 2 expected tokens, repeat.\n",
        "# changing the temperature schedule can result in wildly different output!\n",
        "temperature = [1.0, 0.5, 0.2, 0.2]   \n",
        "prefix = None   # if you want each generated text to start with a given seed text\n",
        "\n",
        "if train_cfg['line_delimited']:\n",
        "  n = 1\n",
        "  max_gen_length = 60 if model_cfg['word_level'] else 300\n",
        "else:\n",
        "  n = 1\n",
        "  max_gen_length = 2000 if model_cfg['word_level'] else 10000\n",
        "  \n",
        "timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "gen_file = '{}_gentext_{}.txt'.format(model_name, timestring)\n",
        "\n",
        "textgen.generate_to_file(gen_file,\n",
        "                         temperature=temperature,\n",
        "                         prefix=prefix,\n",
        "                         n=n,\n",
        "                         max_gen_length=max_gen_length)\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "You can download the weights and configuration files in the cell below, allowing you recreate the model on your own computer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('{}_weights.hdf5'.format(model_name))\n",
        "files.download('{}_vocab.json'.format(model_name))\n",
        "files.download('{}_config.json'.format(model_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "To recreate the model on your own computer, after installing textgenrnn and TensorFlow, you can create a Python script with:\n",
        "\n",
        "```\n",
        "from textgenrnn import textgenrnn\n",
        "textgen = textgenrnn(weights_path='colaboratory_weights.hdf5',\n",
        "                       vocab_path='colaboratory_vocab.json',\n",
        "                       config_path='colaboratory_config.json')\n",
        "                       \n",
        "textgen.generate_samples(max_gen_length=1000)\n",
        "textgen.generate_to_file('textgenrnn_texts.txt', max_gen_length=1000)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk39QQiOIGCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textgen = textgenrnn(weights_path='colaboratory_weights.hdf5',\n",
        "                       vocab_path='colaboratory_vocab.json',\n",
        "                       config_path='colaboratory_config.json')\n",
        " \n",
        "textgen.generate_samples(max_gen_length=1000)\n",
        "textgen.generate_to_file('textgenrnn_texts.txt', max_gen_length=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}